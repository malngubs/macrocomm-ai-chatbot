Macro

Macrocomm Fleet Analytics

Disaster Recovery and Business Continuity Plan

Table of Contents

YT. INTFOCUCTION? «0... eee eee cc ee cc eece teen ccna eeee eee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee esse eeeeeeee seen eeee ees 2
2. SYSTEM OVELVICW! ......cccecc sec eecesceceeceecceeceeeeeceeceeseeeeeceeeseeseeseeceeseeseecseeseeseeseeceesesceeceeeeees 2
3. Disaster Recovery (DR) Personnel Organigram: ...........cccccsccescescescceceeccesceececeecescesceeeeees 4
A.  RiSK ASSESSMENT? ......... cece cece esece seen eeeeeeeee eee eeeeeeeeeeeeeeeeeseeeseeeseeeeeeeeeeeeeeeeeeeseeeeeneeeees 4
5. Disaster RECovery ODjectives:...........ccccceccesceecescscceccecceecseceeceeceeceeceecseeseeecseeseesseseeeeees 4
6. - BACKUP Strategy: .........ceccecceeceeceeceeceecceeceeceeceeceeceeceeceeeseeseeseeceeseeceeceecseeseesecessescoeseeseees 4
7. Disaster Recovery Procedures: ..........cccceceecscceceececeececeececeececceceeseceececececeecseeesceeseseeeess 5

7.1 Hardware Failure — Bare Metal .......... cc cececesecsecceeceeeceeeeeeeeeeeeeeeeeeeceeeeeseeeseeeseee seen ees 5

7.2 Hardware Failure — Virtual Machine ........... cee cececeseceeeceeeceeeceeeeeeeeeeeceeeeeeeeeeeeeeseee seen ees 6

7.3 Software Failure ......... ee eceeeeccceseccneececeeecceeseeeesceeeseceneseeeeseeenseeeeesesenseeeeeseneeeeene rena 7

7.4 Kubernetes Failure 2.0.0... eceesec reece eeceeeceeeceeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeseeeseeeeeeeeee ees 7
8. Cybersecurity Threats.............cccccescssccseceeceeceeceecesceeccsesesseceeceeceeseeceecsseseeseceesesceeceeeeees 9

8.1 Ransomware Attack Response Plan (Hosted Servers) .........c.ssssseccceesseecceesceseaeeseeenanees 9

8.2 DDoS Attack Response & Mitigation Plan.............ccccccccssssecccsesseeccseesseccsenssecesaeeseeeseees 11
9. Data Centre DiSrUptions .............ccccccccecceeceeceeceeceeccecceeseececeecesceeceecseeseseeceecessesceeeeees 12
10. HUMAN EFrOr..... se eeeeeecsecceeceeeceeeceeeceeeceeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeseeeeeeeeeeeeeeeeeeeee seen ees 13
11. COMMUNICATION PLAN .......... ee ee eeeeeceeeeeeceeeccneeeeceeeceeeseeeseeeseseeeeseseneseeeeseeeeeeeeerenee 13
12. Testing ANd MAINTENANCE ............cccecceceececeeceececeececeececeececeeceeeeceeseceseceececeeceseeceseeees 13
13. CONCLUSION ........... ce ceeceesecceeesccenscceeesenenseceeeseeeeseceeeceeeseeeeeeeeeseeeeseseneseeeeseeenseeeerenees 14

Ref: MFA-DISASTER RECOVERY AND BUSINESS CONTINUITY PLAN 202504.03

macro

1. Introduction:

MFA operates a SaaS-based fleet analytics system that serves to assist commercial
fleet owners in the day-to-day optimisation of their fleet operations. Trip data, driver
behaviour data and cost data are ingested from multiple sources, via multiple
means, for processing and analysis, delivering actionable insights around fleet
optimisation.

Considering the sensitivity of the data, legislation governing the flow and storage of
data and the reliance of MFA clients on the business-critical insights revealed by the
analytics SaaS, this Disaster Recovery Plan (DRP) outlines the necessary procedures
and protocols to be followed in the event of a disaster affecting the Macrocomm
Fleet Analytics (MFA) fleet management system. It ensures minimal downtime, data
integrity, and business continuity.

Unless otherwise indicated/detailed, Dev Ops is appointed Single Point of Contact
(SPOC) and communications procedures are as stipulated in section 11.
Communication Plan.

2. System Overview:

The system architecture diagram below serves to depict the tech stack, data flows,
methodologies employed to secure data in transit and data at rest, and
systems/services employed to mitigate risks associated with data security as well as
failovers to mitigate system failures.

Ref: MFA-DISASTER RECOVERY AND BUSINESS CONTINUITY PLAN 202504.03

“\macrocomm

Afrihost
aa MTN Data Centre - Gallo Manor
2FA (Q)
auth “ ©)
6
Dev/Admin
macrocomm —A@| https://
Fleet Analytics SSH |
Third Party
€Q | https:// Data
. 6354
Table backups Database
every 10 256 backups <“y
minutes every 24 hrs

Fortinet Firewall

Afrihost
IS Data Centre
Rosebank

Macocomm —A@l https://

Fleet Analytics

fm

Third Party
Data

€8 | https://

e The three servers are housed within data centres, gaining advantage from 99.9%
uptime SLAs with the data centre providers.

e The Production Server stands alone from the Development & Staging Server and
Mail Server.

e The Production Server is a VM, benefiting from non-disruptive scalability of
processing power, RAM and disk storage.

e The Staging Server - housed in a geographically separated data centre - mirrors
the Production Server as a failover in the event of a catastrophic failure at
Production Server level.

e Source/third party data is ingested via API calls, push to SFTP or delivery of CSV
or XLSX files as email attachments to bespoke mailboxes, per client, on the Mail
Server.

e Data in transit is over SSH secured connections, and in the event of email
transfer, TLS 2 encrypted.

e Data at rest is AES-256 encrypted.

e Database tables are backed up, incrementally, from the Production Server to the
Staging Server every 10 minutes.

e The entire database is backed up from the Production Server to the Staging
Server every 24 hours.

e Developer/Admin access to all servers is via VPN, with Role-Based Access Control
(RBAC) strictly enforced to ensure least privilege access.

Ref: MFA-DISASTER RECOVERY AND BUSINESS CONTINUITY PLAN 202504.03

macro

e Ingested data is normalised via Python scripts and processed by a combination
of Python scripts and Postgre SQL stored procedures.

e Client access to processed/aggregated data is delivered over the internet by the
Production Server, housing its own web server in the form of Nginx, via Angular
making secure API calls to the Production Server through a Docker containerised
FastAPI layer.

e Inbound client connections are Multi Factor Authentication controlled via the
Auth0 cloud service.

e Inbound connections to all servers pass through a Fortinet Firewall.

3. Disaster Recovery (DR) Personnel Organigram:

Data Security Officer (DSO)

Head Developer | Database Administrator | Server Administrator

Dev Ops | Client Liaison Officer (CLO)

4. Risk Assessment:

e Server failure - bare metal (storage, NIC, RAM, processor, motherboard,
automatic update to OS).

e Software failure (database corruption, patch/upgrade failure).

e Cybersecurity threats (ransomware, DDOS attacks, unauthorised access, data
breach).

e Data centre disruptions (fire, flooding, power outages).

e Human error (accidental deletion, misconfigurations).

5. Disaster Recovery Objectives:

e RTO (Recovery Time Objective): Maximum allowable downtime is 4 hours.
e RPO (Recovery Point Objective): Maximum allowable data loss is 10 minutes.

6. Backup Strategy:

e Database tables are incrementally backed up from the Production Server to the
Staging Server every 10 minutes.

e Full database backups from the Production Server to the Staging Server are
executed, after 00:00, every 24 hours.

e The Staging Server database tables are updated from the 10-minute incremental
backups to effect a full failover environment in the event of a catastrophic
Production Server failure. Live synching too risky in terms of SQL compromise or
ransomware.

Ref: MFA-DISASTER RECOVERY AND BUSINESS CONTINUITY PLAN 202504.03

7.

macro

A manual A-pointer change will be implemented to redirect inbound client traffic,
once it is established that such a re-direct will not compromise the failover server
- maximum 1 hour.

A full-tin backup image of the Staging Server is made to a Multi Factor
Authentication protected location on the Production Server, after 03:00, every 24
hours, to facilitate a rebuild of a new bare metal Staging Server in the event of a
catastrophic Staging Server failure - maximum 4 hours.

Automated daily reports, via email and Telegram, to Dev Ops confirming the
daily backups have run successfully and detailing the creation timestamp and
size of backup files, along with remaining storage space on the servers.

Any failure in the backup routines to be thoroughly investigated by Dev Ops,
rectified, as well as documented, using the Server Exception report template,
detailing the failure and remedy and distributed via email to all members of the
DR team.

Backup files are archived for 5 years.

Disaster Recovery Procedures:

7.1 Hardware Failure — Bare Metal

Dev Ops to immediately inform all members of the DR team of the failure and
enlist all available DR team members, as well as data centre support agents, to
identify the nature of the failure.

Dev Ops to immediately open a DR Exception report and record the date and
time of the occurrence.

Dev Ops to identify the nature of the failure and engage the specialist member/s
of the DR team, commensurate with the nature of the failure, who will run lead
during the implementation of the remedy.

Dev Ops to appoint a Single Point of Contact (SPOC) from the DR specialist team.
SPOC to determine/estimate the timespan for remedial action and communicate
the cause and remedy to the rest of the DR team.

Re-direct incremental database tables to a suitable local machine
controlled/owned by one of the DR team.

CLO to inform impacted staging clients, via email, if services cannot be restored
within 5 minutes.

CLO to inform impacted staging clients, via email, of anticipated downtime.
SPOC to, if necessary, engage the data centre to spin up a new server, restoring
the latest full-tin backup as soon as the new server goes live.

SPOC to manually synch the databases once the new server has been fully
restored.

Appointed lead DR team member to manually run any automated backups that
were missed while the old server was down.

CLO to inform impacted staging clients, via email, that services are restored.
SPOC to complete the Server Exception Report, accurately detailing the cause of
failure, the remedial action taken, as well as the course of action to be
implemented prevent such a failure from reoccurring.

Ref: MFA-DISASTER RECOVERY AND BUSINESS CONTINUITY PLAN 202504.03

macro

SPOC to submit the Server Exception report to the DSO.

DSO to document and implement any change controls that may arise from the
failure as well as update the DR plan, in accordance with ISO guidelines
surrounding change control and policy document updates.

7.2 Hardware Failure — Virtual Machine

Dev Ops to immediately inform all members of the DR team of the failure and
enlist all available DR team members, as well as data centre support agents, to
identify the nature of the failure.

Dev Ops to immediately open a DR Exception report and record the date and
time of the occurrence.

Dev Ops to identify the nature of the failure and engage the specialist member/s
of the DR team, commensurate with the nature of the failure, who will run lead
during the implementation of the remedy.

Dev Ops to appoint a Single Point of Contact (SPOC) from the DR specialist team.
SPOC to determine/estimate the timespan for remedial action and communicate
the cause and remedy to the rest of the DR team.

Re-direct incremental database tables to a suitable local machine
controlled/owned by one of the DR team.

CLO to inform impacted clients, via email, if services cannot be restored within 5
minutes.

CLO to inform impacted staging clients, via email, of anticipated downtime.
SPOC to, if necessary, engage the data centre to spin up a new server, restoring
the latest full-tin backup as soon as the new server goes live.

If the entire data centre is down, SPOC to engage an alternate data centre -
listed on https://www.datacentermap.com/south-africa/johannesburg/ - to spin
up a VM of identical or higher specification and restore the latest full-tin backup
as soon as the new server goes live.

SPOC to manually synch the databases once the new server has been fully
restored.

SPOC to manually set the A pointer of the domain to the new server IP address
and set the TTL to 300 seconds on
https://www.domains.co.za/client/services/domains/manage-dns/103506.

SPOC to manually run any automated backups that were missed while the old
server was down.

CLO to inform impacted staging clients, via email, that services are restored.
SPOC to complete the DR Exception report, accurately detailing the cause of
failure, the remedial action taken, as well as the course of action to be
implemented prevent such a failure from reoccurring.

SPOC to submit the DR Exception report to the DSO.

DSO to document and implement any change controls that may arise from the
failure as well as update the DR plan, in accordance with ISO guidelines
surrounding change control and policy document updates.

Ref: MFA-DISASTER RECOVERY AND BUSINESS CONTINUITY PLAN 202504.03

macro

7.3 Software Failure

Dev Ops to immediately inform all members of the DR team of the failure and
enlist all available DR team members to identify the nature of the failure.

Dev Ops to immediately open a DR Exception report and record the date and
time of the occurrence.

Dev Ops to identify the nature of the failure and engage the specialist member/s
of the DR team, commensurate with the nature of the failure, who will run lead
during the implementation of the remedy.

Dev Ops to appoint a Single Point of Contact (SPOC) from the DR specialist team.
SPOC to determine/estimate the timespan for remedial action and communicate
the cause and remedy to the rest of the DR team.

CLO to inform impacted clients, via email, if services cannot be restored within 5
minutes of the estimated downtime.

SPOC to complete the DR Exception report, detailing the cause and remedy as
well as total downtime and implementations to prevent recurrence.

Appointed lead DR team member to submit the DR Exception report to the DSO.
DSO to document and implement any change controls that may arise from the
failure as well as update the DR plan, in accordance with ISO guidelines
surrounding change control and policy document updates.

7.4 Kubernetes Failure

Recovery Scenarios & Tailored Actions

Node Failure
Trigger: Physical host crash, Ubuntu node OS corruption
Actions:
o Provision a new Ubuntu VM via MTN Gallo Manor.
o Install Kubernetes and join the node using:

bash Copy @ Edit
kubeadm <control-plane>:6443 --token <token> ..

Kubernetes reschedules pods automatically.
Verify:

bash O Copy @ Edit
kubectl get nodes -o wide

Make sure PVs are backed by non-node-specific storage (e.g., NFS, CSI).

Frontend Failure (Angular + Nginx)

Symptoms: Angular app inaccessible, Nginx misroutes.

Actions:

Redeploy Angular container from GitLab Cl pipeline or Helm chart.
Restore or redeploy Nginx ingress config:

Ref: MFA-DISASTER RECOVERY AND BUSINESS CONTINUITY PLAN 202504.03

\macro

bash O Copy @ Edit
kubectl apply -f ingress.yaml

e Test:

bash © Copy @ Edit

curl -I https://yourdomain.com

etcd or Control Plane Failure

e Symptoms: kubectl fails; cluster completely non-functional.
e Actions:

e SSH to control plane node, stop kubelet.

e Restore etcd from recent snapshot:

bash O Copy @ Edit

ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \
--data-dir /var/lib/etcd-restored

e Point kube-apiserver to the restored data directory.
e Restart control plane services:

bash O Copy @ Edit

systemctl restart kubelet

Namespace or Microservice Deletion
e Trigger: Accidental kubectl delete ns prod-backend
e Actions:

o Redeploy via GitOps/Helm:

bash O Copy @ Edit

helm upgrade --install backend ./charts/backend -n prod

e Restore PVs using Velero:

bash O Copy © Edit

velero restore create --from-backup backend-prod-backup

Cluster-Wide Failure (Disaster)
e Trigger: Data center outage, all VMs lost.
e Actions:
e Provision new VMs at MTN or cloud (use Ansible or Terraform).
e Rebuild Kubernetes with kubeadm init.
e Restore etcd (if available).
e Redeploy manifests from Git/Helm:

Ref: MFA-DISASTER RECOVERY AND BUSINESS CONTINUITY PLAN 202504.03

8.

macro

kubectl apply -f cluster-state/

e Restore volumes with Velero:

bash O Copy @ Edit

velero install

velero restore ...

e Practice full disaster recovery once per quarter in a test environment.

Cybersecurity Threats

8.1 Ransomware Attack Response Plan (Hosted Servers)

Immediate Detection and Containment

Goal: Stop the spread, isolate affected systems, preserve evidence.
Identify signs: Unusual CPU/disk usage, encrypted files, ransom notes, blocked
access.
Disconnect affected server(s) from the network immediately:
Disable NIC (network interface card).
Remove from load balancers or virtual networks.
Disable shared drives/storage mounted from the infected server.
Block external communications (firewall rules, outbound traffic) to prevent data
exfiltration or C2 (command-and-control) contact.
i, Do not reboot or wipe the system yet — preserve evidence for forensic
analysis.

Activate Incident Response Team

Bring together all key personnel and resources.

Notify internal incident response team (IR team).

Escalate to executive management, legal, and PR teams as needed.

Involve forensic analysts (DSO and Cyber Africa).

Appoint a single point of contact (SPOC) for managing communications and
updates.

Forensic Analysis & Impact Assessment

Goal: Understand the scope and identify the entry point.
Preserve a full image of the infected server for analysis.
Review:
Access logs
System event logs (e.g., /var/log/auth.log, /var/log/syslog)
Recent process activity (ps aux, top, history)
o Network traffic
Determine:
o Infection vector (phishing, RDP, unpatched vulnerability)

Oo 0 O

Ref: MFA-DISASTER RECOVERY AND BUSINESS CONTINUITY PLAN 202504.03

macro

o Files and data affected

o Any data exfiltration or unauthorized access
Record file hashes, timestamps, ransom note contents, and any attacker contact
info.

Contain and Eliminate the Threat

Goal: Remove the malware and any persistence mechanisms.
Terminate malicious processes (kill, pkill, top).
Identify and delete:
Malicious binaries/scripts
Unauthorized users or SSH keys
Cron jobs or systemd timers used for persistence
Patch all known vulnerabilities on all affected and adjacent systems.
Rotate credentials:
o System users
o API keys
o Database credentials
o VPN and remote access accounts
Reassess all servers in the hosted environment, even if only one appears
affected.

Restore Operations from Backups

Goal: Recover to a clean, known-good state.

Do NOT restore over the infected server.

Rebuild the server (format disk, reinstall OS and apps).

Apply all security patches.

Verify backup integrity and timestamps.

Restore from offline or immutable backup (not mounted to the infected host).
Scan restored data before putting back into production.

Gradually reintroduce server to the network, under monitoring.

If backups are encrypted or compromised, escalate to cyber insurance/legal.

Communication & Reporting

Goal: Legal compliance, customer assurance, and team coordination.
Notify affected clients (if Pl, financial, or regulated data was exposed).
Report the incident to:
o State Security Agency for criminal investigation
o South African Police Service (SAPS) for criminal investigation
o Information Regulator as required by POPIA
Prepare internal and external statements.
Ensure clear communication with internal staff to avoid rumour spread.

Post-Incident Review & Hardening

Goal: Learn and strengthen defences.
Conduct a root cause analysis.

Ref: MFA-DISASTER RECOVERY AND BUSINESS CONTINUITY PLAN 202504.03

macro

e Review and improve:
o Patch management process
o Access controls (SSH, VPN, admin rights)
o User awareness and phishing training
o Firewall and IDS/IPS rules

8.2 DDoS Attack Response & Mitigation Plan

Initial Triage

e Identify type:
o Layer 3/4 (network) or Layer 7 (application)?
o Small-scale botnet or volumetric?

e Determine source patterns:

o IPs, headers, countries, user-agents

Mitigation (During Attack)
e Network/Infrastructure Level Mitigation
e Engage upstream provider (e.g., MTN, Afrihost, Azure) to:
o Block traffic at their edge
o Apply upstream null-routing / sinkholing
e Enable:
o "Under Attack Mode"
o IP reputation filtering
o Rate limiting rules

e Server/Application-Level Mitigation
e Temporarily enable aggressive rate limits and bot detection in Nginx/FastAPI:

OD Copy @ Edit

imit conn addr 5;

req zone $binary_remote addr zone=api:10m rate=1r/s;

e Use fail2ban to block IPs with malicious patterns.
e Disable endpoints or services being targeted.

e Drop malformed or suspicious packets using iptables:
bash O Copy @ Edit

iptables -A INPUT -p tcp --syn -m -- 1/s -j ACCEPT

Divert/Scale Traffic
e Redirect traffic to cloud-based scrubbing centres.
e Spin up additional instances or containers in alternate zones to absorb traffic.

Ref: MFA-DISASTER RECOVERY AND BUSINESS CONTINUITY PLAN 202504.03

macro

Post-Attack Recovery
e Assess and Restore
o Review logs for origin and extent of attack
o Re-enable temporarily disabled services
o Monitor for residual or repeat attacks
o Revert any temporary rate limits if performance is degraded

Report & Communicate
e Document the event:
o Time of onset and resolution
o Attack type and volume
o Mitigation measures taken
e Notify clients if service was impacted (transparently).
e Report attack to upstream ISP and/or law enforcement if it involved criminal
extortion.

9. Data Centre Disruptions

e Dev Ops to immediately inform all members of the DR team of the failure and
enlist all available DR team members, as well as data centre support agents, to
identify the nature of the failure.

e Dev Ops to immediately open a DR Exception report and record the date and
time of the occurrence.

e Dev Ops to identify the nature of the failure and engage the specialist member/s
of the DR team, commensurate with the nature of the failure, who will run lead
during the implementation of the remedy.

e Dev Ops to appoint a Single Point of Contact (SPOC) from the DR specialist team.

e SPOC to determine/estimate the timespan for remedial action and communicate
the cause and remedy to the rest of the DR team.

e If necessary:

o Re-direct incremental database tables to a suitable local machine
controlled/owned by one of the DR team.

o CLO to inform impacted clients, via email, if services cannot be restored
within 5 minutes.

o CLO to inform impacted staging clients, via email, of anticipated
downtime.

o SPOC to, if necessary, engage the data centre to spin up a new Server,
restoring the latest full-tin backup as soon as the new server goes live.

o If the entire data centre is down, SPOC to engage an alternate data centre
- listed on https://www.datacentermap.com/south-africa/johannesburg/ -
to spin up a VM of identical or higher specification and restore the latest
full-tin backup as soon as the new server goes live.

o SPOC to manually synch the databases once the new server has been
fully restored.

Ref: MFA-DISASTER RECOVERY AND BUSINESS CONTINUITY PLAN 202504.03

10.

12.

macro

o SPOC to manually set the A pointer of the domain to the new server IP
address and set the TTL to 300 seconds on
https://www.domains.co.za/client/services/domains/manage-dns/103506.

o SPOC to manually run any automated backups that were missed while the
old server was down.

e CLO to inform impacted clients, via email, that services are restored.

e SPOC to complete the DR Exception report, accurately detailing the cause of
failure, the remedial action taken, as well as the course of action to be
implemented prevent such a failure from reoccurring.

e SPOC to submit the DR Exception report to the DSO.

e DSO to document and implement any change controls that may arise from the
failure as well as update the DR plan, in accordance with ISO guidelines
surrounding change control and policy document updates.

Human Error

Restore data from backup immediately.
Implement stricter access controls and logging mechanisms.
Conduct additional training for personnel.

Communication Plan

Incident Reporting: All incidents must be reported to the DSO by the SPOC.
Stakeholder Notification: Via email and any additional communications means
deemed necessary, SPOC to notify key stakeholders, including executive
management and CLO to notify affected clients.

Exception Report: This report must be opened ,by Dev Ops/SPOC, immediately any
disruptive exception occurs, completed meticulously, signed and delivered via email
to the DSO.

Regular Updates: SPOC to provide DSO and stakeholders with progress reports
every hour until resolution.

Change Control: DSO to document and implement any change controls that may
arise from exceptions as well as update the DR plan, in accordance with ISO
guidelines surrounding change control and policy document updates.

Testing and Maintenance

Quarterly DR Drills: Simulate disaster scenarios and validate response times.
Annual Full-System Restoration Test: Ensure backups are functional and data
integrity is maintained.

Ongoing Monitoring: Implement automated alerts for potential system failures.

Ref: MFA-DISASTER RECOVERY AND BUSINESS CONTINUITY PLAN 202504.03

macro

13. Conclusion

By following this DRP, MFA ensures business continuity, minimizes downtime, and
protects critical fleet analytics data. Regular reviews and updates will be conducted to
align with evolving risks and technology advancements.

Ref: MFA-DISASTER RECOVERY AND BUSINESS CONTINUITY PLAN 202504.03